---
layout: post
title:  Week 8 | Open Source AI
---

Nick Vidal gave a great presentation on the benefits of open source AI. Coming into the presentation I was a bit skeptical that having the requirement to release all of the data used in training a model would be practical. However, when I asked him a question about it at the end, he explained that what you actually need to do in order to fit the open source ai definition is to just give instructions on how to obtain the same data.

<!--more-->

<img width="300" src="/jpjacobpadilla-weekly/images/week8-ossi.webp" alt="Documentation screenshot">


## Olmo2

One new large language model that fits this open source definition is [OOlmo2](https://allenai.org/blog/olmo2). It’s almost as good as GPT-4o and is completely open. One of the best parts about this is that it means people (like me) can take a really deep dive into every part of the model to see exactly how it all works.

## Llama

That being said, in my opinion, the open source ai definition is still a bit too rigid. A lot of the big companies who are spending billions on research will never be able to comply with it as even just the data component would lead to them getting a lot of lawsuits.

I think this is unfortunate since there is some great open research going on at companies like Meta that a lot of other developers and researchers find helpful. Just because it doesn’t fit the OSI’s strict definition, I would still consider models like Llama to be open source since anyone can easily access the model’s weights on hugging face. 
