---
layout: post
title:  Week 8 | Open Source AI
---

Nick Vidal gave a great presentation on the benefits of open source AI during class! Coming into the presentation I was a bit skeptical that having the requirement to release all of the data used in training a model would be practical. However, when I asked him a question about it, he explained that what you actually need to do in order to fit the Open Source AI definition is to just give instructions on how to obtain the same data.

<!--more-->

## Olmo2

One new large language model that fits this open source definition is [Olmo2](https://allenai.org/blog/olmo2). It’s almost as good as GPT-4o and is completely open! One of the best parts about this is that it means people (like me) can take a really deep dive into every part of the model to see exactly how it all works.

## Llama

That being said, in my opinion, the Open Source AI definition is still a bit too rigid. A lot of the big companies who are spending billions on research will never be able to comply with it even though they are doing a lot for the open source community.

I think this is unfortunate since there is some great open research going on at companies like Meta that a lot of other developers and researchers find helpful. Just because it doesn’t fit the OSI’s strict definition, I would still consider models like Llama to be open source since anyone can easily access the model’s weights on hugging face. 

## The Future of Open Source AI

While people can argue over the definition of what it means for a large language model to be "open source," I think most people can agree that ensuring everyone has access to these very powerful models in the future is incredibly important. We wouldn't want to live in a world where a handful of big tech companies literally hold the key to artificial consciousness.

<img width="300" src="/jpjacobpadilla-weekly/images/week8-ossi.webp" alt="Documentation screenshot">
